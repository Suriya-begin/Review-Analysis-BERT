# -*- coding: utf-8 -*-
"""Copy of PRSA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YmsmP3WJqUvldAv4lIH9Kt0_M1ClUWrn
"""

!pip install transformers datasets --quiet

import pandas as pd
import torch
from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import glob

!pip install transformers

import pandas as pd

folder_path = '/content/drive/MyDrive/imdb/'


train_path = folder_path + 'train-00000-of-00001.parquet'
test_path = folder_path + 'test-00000-of-00001 (1).parquet'
unsup_path = folder_path + 'unsupervised-00000-of-00001.parquet'

df_train = pd.read_parquet(train_path)
df_test = pd.read_parquet(test_path)
df_unsup = pd.read_parquet(unsup_path)

print("Train Data:")
display(df_train.head())

print("Test Data:")
display(df_test.head())

print("Unsupervised Data:")
display(df_unsup.head())

all_texts = pd.concat([
    df_train['text'],
    df_test['text'],
    df_unsup['text']
], ignore_index=True)

import re

def clean_text(text):
    text = text.lower()
    text = re.sub(r'<br />', ' ', text)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

all_texts_cleaned = all_texts.apply(clean_text)

from transformers import BertTokenizer, BertModel
import torch
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

train_encodings = tokenizer(df_train['text'].tolist(), truncation=True, padding=True)
test_encodings = tokenizer(df_test['text'].tolist(), truncation=True, padding=True)

import torch

class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = IMDbDataset(train_encodings, df_train['label'].tolist())
test_dataset = IMDbDataset(test_encodings, df_test['label'].tolist())

from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=8,   # batch size per device during training
    per_device_eval_batch_size=8,    # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10     # evaluate each epoch
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()
# Evaluate the model
trainer.evaluate()
from sklearn.metrics import accuracy_score, classification_report

labels = df_test['label'].tolist()
print("Accuracy:", accuracy_score(labels, preds))
print(classification_report(labels, preds))
predictions = trainer.predict(test_dataset)
preds = torch.argmax(torch.tensor(predictions.predictions), axis=1)

from sklearn.metrics import accuracy_score, classification_report

labels = df_test['label'].tolist()
print("Accuracy:", accuracy_score(labels, preds))
print(classification_report(labels, preds))

model.save_pretrained("./bert-finetuned")
tokenizer.save_pretrained("./bert-finetuned")

trainer.push_to_hub("my-finetuned-bert")

model.save_pretrained("/content/drive/MyDrive/bert-finetuned")
tokenizer.save_pretrained("/content/drive/MyDrive/bert-finetuned")

predictions = trainer.predict(test_dataset)
preds = torch.argmax(torch.tensor(predictions.predictions), axis=1)


print("Accuracy:", accuracy_score(labels, preds))
print(classification_report(labels, preds))
predictions = trainer.predict(test_dataset)
preds = torch.argmax(torch.tensor(predictions.predictions), axis=1)

from sklearn.metrics import accuracy_score, classification_report

labels = df_test['label'].tolist()
print("Accuracy:", accuracy_score(labels, preds))
print(classification_report(labels, preds))

model.save_pretrained("./bert-finetuned")
tokenizer.save_pretrained("./bert-finetuned")

model.save_pretrained("/content/drive/MyDrive/bert-finetuned")
tokenizer.save_pretrained("/content/drive/MyDrive/bert-finetuned")

from transformers import BertForSequenceClassification, BertTokenizer

model = BertForSequenceClassification.from_pretrained("/content/drive/MyDrive/bert-finetuned")
tokenizer = BertTokenizer.from_pretrained("/content/drive/MyDrive/bert-finetuned")

text = "Let me tell u this in the beginning itself. This is one of the worst Tamil movies ever made. And wait. Some one else told here that this movie deserves and Oscar. Please man. I hope you 've out of asylum. This is movie was completely senseless. Vijay has no expressions on his face and is completely dull. Poor screenplay. Poor direction. In fact I think he dint even direct the movie. Worst music. Except one song every other songs in the movie seem to be Kuthu. Can some one please explain me which music director will put such songs for a fantasy movie. And the other actors are also completely wasted. No one is given importance in this film . Just a usual masala flick with nonsense written all over it"  # Replace with your test text
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)

model.eval()
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    predicted_class = logits.argmax(dim=1).item()

label_map = {0: "negative", 1: "positive"}
predicted_label = label_map[predicted_class]

print("Predicted label:", predicted_label)



